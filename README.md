# Reformer---Efficient-Text-Generation---Pushing-the-Limits-of-Language-Modeling

This project demonstrates the Reformer model (Kitaev, Kaiser, Levskaya) â€” a Transformer variant optimized for long-sequence modeling with reduced memory consumption.

We train Reformer on Crime and Punishment (a novel with over 500,000 tokens) to highlight its efficiency and scalability compared to standard Transformers.

ðŸ“Œ Problem

Traditional Transformers suffer from quadratic time and memory complexity with sequence length, making them impractical for extremely long texts (books, documents, etc.).

ðŸš€ Methodology

Reformer introduces several innovations to address these limitations:

âš¡ LSH Attention â€“ Approximates attention using locality-sensitive hashing, reducing complexity from O(LÂ²) â†’ O(L log L).

â™» Reversible Residual Layers â€“ Saves GPU memory by recomputing activations during backpropagation.

ðŸ§© Axial Positional Embeddings â€“ Efficiently encodes positions for very long sequences.

ðŸ”„ Mixed Attention (Local + LSH) â€“ Balances local dependencies with global scalability.

ðŸ“Š Key Claims

Processes sequences >500K tokens on a single GPU.

Achieves comparable accuracy to full self-attention models.

Provides up to 40Ã— speedup on long-sequence benchmarks.

ðŸ“š Learning Highlights

âœ… Memory savings via reversible layers and chunked feed-forward activations.

âœ… Performance parity with full attention models on character-level language modeling benchmarks.

âœ… Demonstrated efficiency on Crime and Punishment (~800 pages).
