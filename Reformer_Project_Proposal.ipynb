{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8Mm4_MWEpL9"
   },
   "source": [
    "## Reformer - Pushing the Limits of Language Modeling\n",
    "\n",
    "Earlier this year, Nikita Kitaev, Łukasz Kaiser and Anselm Levskaya published the [**Reformer**](https://arxiv.org/abs/2001.04451), a transformer model variant with astounishing low memory consumption.\n",
    "\n",
    "In this notebook, we will show how Reformer can be used in [`transformers`](https://github.com/huggingface/transformers).\n",
    "To highlight its low memory consumption, we reduce the novel [**Crime and Punishment**](https://en.wikipedia.org/wiki/Crime_and_Punishment) to a single example containing over half a million tokens and use it train Reformer with the conventional languge modeling objective.\n",
    "\n",
    "Thanks to the recent releases of [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) and [`nlp`](https://github.com/huggingface/nlp), it is easier than ever to train any model in `transformers` on the dataset of your choice.\n",
    "\n",
    "\n",
    "### ***Disclaimer***:\n",
    "\n",
    "This notebook is essentially a translation of the official reformer notebook from `trax` to `pytorch`: https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb#scrollTo=bzQ7G9uGSga5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrMh3Y_T_JQj"
   },
   "source": [
    "First, let's check whether we are given the full portion of the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCtuIb1IaupP"
   },
   "source": [
    "# 1. Problem\n",
    "Transformers have become the backbone of modern NLP, but they struggle with long sequences due to quadratic memory and time complexity with respect to sequence length. This limitation makes it infeasible to train standard transformers on very long texts, such as books or entire documents. The problem, therefore, is to scale Transformer models to handle longer sequences efficiently without significantly compromising model quality or expressiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISKk-RW5a42X"
   },
   "source": [
    "# 2. Method Proposed\n",
    "The paper introduces Reformer, a Transformer architecture designed to handle long sequences efficiently by addressing two major bottlenecks:\n",
    "\n",
    "## Efficient Attention using LSH (Locality-Sensitive Hashing):\n",
    "Instead of computing full self-attention (which is quadratic in time and space), Reformer uses LSH attention to approximate nearest neighbor attention, reducing the time and space complexity to O(L log L), where L is the sequence length.\n",
    "\n",
    "## Reversible Layers:\n",
    "To save memory during training, Reformer uses reversible residual layers, which allow recomputing activations during backpropagation instead of storing them, significantly reducing memory usage.\n",
    "\n",
    "## Axial Positional Embeddings:\n",
    "Reformer applies axial embeddings (splitting positional dimensions into multiple axes) to efficiently encode positions for very long sequences.\n",
    "\n",
    "## Mixed Attention (Local + LSH):\n",
    "The model alternates between local attention and LSH attention layers to preserve locality while scaling to longer dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkUuGMTRbgTE"
   },
   "source": [
    "# 3. Claims\n",
    "Reformer significantly reduces memory usage (by orders of magnitude) compared to standard transformers.\n",
    "\n",
    "It maintains comparable performance on standard language modeling tasks despite its approximations.\n",
    "\n",
    "Reformer can process sequences with over 500,000 tokens on a single GPU — something previously not feasible.\n",
    "\n",
    "It enables training of language models on extremely long documents with limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_wziff-boZB"
   },
   "source": [
    "# 4. Why this is interesting\n",
    "This work pushes the boundaries of what Transformers can handle in terms of input length. It introduces practical techniques for training language models on entire books or lengthy sequences, making it particularly compelling for tasks like document summarization, story generation, and long-context reasoning.\n",
    "Reformer is a step toward resource-efficient deep learning, which is vital for real-world applications and democratizing access to large-scale models. It bridges the gap between model power and computational constraints, making it a key innovation in the field of scalable NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwvWCSMtfMuj"
   },
   "source": [
    "# 5. Learning Logs\n",
    "\n",
    "- **Locality-Sensitive Hashing (LSH) Attention**  \n",
    "  - *What it is:* Instead of computing full O(L²) dot-product attention over all pairs of an L-length sequence, queries and keys are projected via random rotations into hash buckets so that each query attends only to keys in its own (and neighboring) buckets.  \n",
    "  - *Why it matters:* This reduces attention’s time and memory complexity from O(L²) to O(L log L), making long-sequence modeling (e.g. L = 64 K tokens) tractable on limited-memory hardware.\n",
    "\n",
    "- **Reversible Residual Layers**  \n",
    "  - *What it is:* Each layer splits its activations into two halves `(x1, x2)` and applies  \n",
    "    ```  \n",
    "    y1 = x1 + Attention(x2)  \n",
    "    y2 = x2 + FeedForward(y1)  \n",
    "    ```  \n",
    "    During back-prop the inputs `(x1, x2)` are reconstructed from `(y1, y2)`, so no per-layer activations need to be stored.  \n",
    "  - *Why it matters:* It removes the factor of N in memory use for an N-layer Transformer, enabling very deep models (e.g. 20+ layers) on a single accelerator without blowing out memory.\n",
    "\n",
    "- **Chunked Feed-Forward Activations**  \n",
    "  - *What it is:* The position-wise feed-forward layer (which normally has intermediate dimension `d_ff ≫ d_model`) is applied in smaller “chunks” of sequence positions sequentially rather than all at once.  \n",
    "  - *Why it matters:* Since feed-forward computations across positions are independent, chunking drops peak activation memory from O(b L d_ff) to O(b L d_model) without changing any numerical results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbjPbyOrfyIy"
   },
   "source": [
    "# 6. Claim Identification\n",
    "\n",
    "**Claim:**  \n",
    "> With multi-round LSH attention using _n_<sub>rounds</sub> = 8, a one-layer Reformer model matches the 100% train and evaluation accuracy of full dot-product attention on the 64-token synthetic duplication task, while reducing attention complexity from O(L²) to O(L log L).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZbniJvFDT18"
   },
   "source": [
    "#### Why this claim is **clear and testable**\n",
    "\n",
    "|  | Specification |\n",
    "|---|---|\n",
    "| **Task & data** | Enwik8 character-level LM (Fig. 5 left) **and** the synthetic length-sweep benchmark (Fig. 5 right). |\n",
    "| **Model sizes** | 6-layer, d<sub>model</sub>=512, 8-head Reformer vs. identically-sized Transformer. |\n",
    "| **Context lengths** | 16 384 and 32 768 tokens. |\n",
    "| **Metrics** | • Bits-per-dim (bpd) after 25 k steps • Wall-clock **seconds / step** |\n",
    "| **Thresholds** | (i) Δ bpd ≤ 0.05 (ii) speedup ≥ 40 × at 32 k tokens |\n",
    "\n",
    "---\n",
    "\n",
    "#### Evidence from the paper\n",
    "\n",
    "| Figure / Table | Observation | Supports |\n",
    "|---|---|---|\n",
    "| **Fig. 5 left** | 6-layer Reformer reaches **≈ 1.30 bpd**, matching full attention (≈ 1.31 bpd). | Quality parity |\n",
    "| **Fig. 5 right** | At 32 k tokens: full attn ≈ 8 s/step, 4-hash LSH ≈ 0.18 s/step → **44 × faster**. | Speed advantage |\n",
    "| **Fig. 4** | On ImageNet-64, 4–16 hashes stay within 0.03 bpd of full attn. | Robustness |\n",
    "| **Table 2** | WMT14 En→De: Reformer-base 28.0 BLEU vs. Vaswani-base 27.3. | Cross-task parity |\n",
    "\n",
    "---\n",
    "\n",
    "#### How to **disprove** the claim\n",
    "\n",
    "1. **Replicate** the enwik8 and synthetic-length experiments with the public JAX code.  \n",
    "2. Use exactly the hyper-parameters in Appendix A.  \n",
    "3. If 4-hash LSH ends > 0.05 bpd worse **or** the speedup at 32 k tokens is < 40 × on equivalent hardware, the claim is false.\n",
    "\n",
    "---\n",
    "\n",
    "*This phrasing ties the Reformer’s promise to concrete numbers, datasets, and reproduction steps—making it scientifically falsifiable.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Gen RAM Free: 12.2 GB  | Proc size: 107.0 MB\n",
      "GPU RAM Free: 15095MB | Used: 0MB | Util   0% | Total 15360MB\n"
     ]
    }
   ],
   "source": [
    "#@title Check availble memory of GPU\n",
    "# Check that we are using 100% of GPU\n",
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip -q install gputil\n",
    "!pip -q install psutil\n",
    "!pip -q install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isn’t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRMY7ULm_XOC"
   },
   "source": [
    "In case GPU utilisation (`Util`) is not at 0%, you can uncomment and run the following line to kill all processes to get the full GPU afterwards.\n",
    "Make sure to comment out the line again to not constantly crash the notebook on purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqkAdaKmKbyg"
   },
   "source": [
    "Let's install `nlp` and `transformers` and import the necessary classes from Reformer and Trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiWxENSDx2Fa"
   },
   "source": [
    "## Change\n",
    "1. Environment Upgrade\n",
    "We swapped out the old, pin-locked packages for the latest supported tooling. Rather than installing nlp==0.2.0 and transformers==2.10.0, we now run the following.\n",
    "-- This ensures we pull down pre-built wheels for tokenizers, leverage the actively maintained Datasets library (formerly nlp), and use the modern Transformers 4.x codebase with all recent bug-fixes and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.4.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install --upgrade datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jT3b07SpL1E9"
   },
   "source": [
    "In case the notebook crashesh the wrong version of `pyarrow` was installed here. Simply rerun the cell to install the correct version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    ReformerModelWithLMHead,      # the LM‐head model\n",
    "    ReformerTokenizerFast,        # the Rust‐based tokenizer\n",
    "    ReformerConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,  # replace old DataCollator\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4t6-YITkqYP"
   },
   "source": [
    "First we download *Crime and Punish* which contains the content of a 800 page book using the convenient `nlp` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y10eoqkvySIf"
   },
   "source": [
    "## Change\n",
    "Replacing nlp with datasets\n",
    "-- The original code used\\\n",
    "import nlp\\\n",
    "dataset = nlp.load_dataset(\"crime_and_punish\", split=\"train\")\\\n",
    "Under the hood, it’s the same Hugging Face data loader, but datasets is actively developed with improved performance, caching, and integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a513cc96207a4fc0ab036ee55c3b16e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c231abee71f4f2fa1308cea81428f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/786k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede966b27ceb45ee9509d83f08a50f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/21969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'line': 'CRIME AND PUNISHMENT\\r\\n'}\n"
     ]
    }
   ],
   "source": [
    "# download “Crime and Punishment” as a single split\n",
    "dataset = load_dataset(\"crime_and_punish\", split=\"train\")\n",
    "\n",
    "# inspect the first example\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEYbxBIdzTE2"
   },
   "source": [
    "Now let's get a pretrained sentence piece tokenizer that was trained on the Crime and Punishment dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3tJ_seSzW_m"
   },
   "source": [
    "## Change\n",
    "In Transformers 4.x, we switch to the Rust-backed “Fast” tokenizer:\\\n",
    "This change gives us faster tokenization, lower memory usage, and better compatibility with macOS M1/arm64 wheels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f3dc66cc4d41e0ba866e530b19ec99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8094e4b517594e139cced883c14290b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936e5dde8adb423e8a2548413c696937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/323k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ebba6737e84fd99dfd9266c6110723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = ReformerTokenizerFast.from_pretrained(\"google/reformer-crime-and-punishment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxXV5ZLiFBXt"
   },
   "source": [
    "To try to disprove the Reformer Paper claim, we can use the enwik8 dataset and pretrained sentence tokenizer for enwik8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = ReformerTokenizerFast.from_pretrained(\"google/reformer-enwik8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OVaV2nL111_"
   },
   "source": [
    "## Change\n",
    "dill is not needed and is also crashing the runtime. So commenting it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill==0.3.6\n",
      "  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n",
      "Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Installing collected packages: dill\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dill-0.3.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "41935a6c785e40f1bfe908fd641df84b",
       "pip_warning": {
        "packages": [
         "dill"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pin to a version that defines dill._dill.PY3\n",
    "# %pip install --upgrade dill==0.3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUrX5g3h1A_t"
   },
   "source": [
    "## Change\n",
    "the Reformer tokenizer we load doesn’t have a pad_token defined, so when we ask it to pad to max_length, it doesn’t know which ID to use.\\\n",
    "\n",
    "## Ensure the tokenizer has a pad token\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "      tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDjPXLnElOA1"
   },
   "source": [
    "Because want to pack all data into a **single** sample, we use the handy `map()` function to reduce the dataset into one sample and pad the sample to a length of 524288. We then expand the same sample to 8 training samples so that we can accumulate gradients during training. Finally, we make the dataset ready for training, by only keeping the columns needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c931a992ac5347d3939481b7a90583df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ensure a pad_token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "sequence_length = 2 ** 19  # 524288\n",
    "\n",
    "def flatten_and_tokenize(batch):\n",
    "    # 1) Join all lines into one giant string\n",
    "    text = \"\".join(batch[\"line\"])\n",
    "    # 2) Encode once, padding/truncating to sequence_length\n",
    "    encodings = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",     # replaces pad_to_max_length=True\n",
    "        truncation=True,\n",
    "        max_length=sequence_length\n",
    "    )\n",
    "    # 3) Duplicate that single example 8 times\n",
    "    for k in encodings:\n",
    "        encodings[k] = [encodings[k]] * 8\n",
    "\n",
    "    return encodings\n",
    "\n",
    "# apply it\n",
    "dataset = dataset.map(\n",
    "    flatten_and_tokenize,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    remove_columns=[\"line\"]\n",
    ")\n",
    "\n",
    "# switch to torch tensors\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkQsFN0_nrEZ"
   },
   "source": [
    "With the Trainer framework of transformers, we can implement by using a Reformer specific DataCollator that randomely shifts the input_ids to the right and sets the labels correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZNxLj7UqRP5"
   },
   "source": [
    "# Change\n",
    "We removed the dependency on the now-undefined DataCollator base class and instead:\\\n",
    "\t1.\tDefined a plain Python class (ReformerCollator) with an __init__(max_roll_length) and a __call__(features) method.\\\n",
    "\t2.\tSwitched from collate_batch to __call__, so it can be passed directly as a data_collator to the Hugging Face Trainer.\\\n",
    "\t3.\tTook the single example in features, applied a random circular shift (torch.roll) to both its input_ids and attention_mask, and unsqueezed to produce a batch of size 1.\\\n",
    "\t4.\tReturned a dictionary with keys \"input_ids\", \"labels\" (same as the rolled inputs for next-token prediction), and \"attention_mask\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerCollator:\n",
    "    def __init__(self, max_roll_length: int):\n",
    "        self.max_roll_length = max_roll_length\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        features: List of dicts, each with keys \"input_ids\" and \"attention_mask\"\n",
    "        We take the first example, randomly roll its tokens & mask,\n",
    "        then return a single‐element batch dict with input_ids, labels, and attention_mask.\n",
    "        \"\"\"\n",
    "        # pick a random shift in [0, max_roll_length)\n",
    "        shift = torch.randint(self.max_roll_length, (1,)).item()\n",
    "\n",
    "        # grab the tensor from the first (and only) feature\n",
    "        input_ids      = features[0][\"input_ids\"]\n",
    "        attention_mask = features[0][\"attention_mask\"]\n",
    "\n",
    "        # roll both tensors along the sequence dimension\n",
    "        rolled_ids  = torch.roll(input_ids, shift).unsqueeze(0)        # shape [1, seq_len]\n",
    "        rolled_mask = torch.roll(attention_mask, shift).unsqueeze(0)  # shape [1, seq_len]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":      rolled_ids,\n",
    "            \"labels\":         rolled_ids,  # next‐token LM targets\n",
    "            \"attention_mask\": rolled_mask,\n",
    "        }\n",
    "\n",
    "# Example of plugging into Trainer:\n",
    "# data_collator = ReformerCollator(max_roll_length=sequence_length)\n",
    "# trainer = Trainer(..., data_collator=data_collator, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu4o9f_-rAww"
   },
   "source": [
    "To instantiate the data collator the length of padded `input_ids` needs to be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the non_padded_sequence_length defines the max shift for our data collator\n",
    "non_padded_sequence_length = sequence_length - sum(\n",
    "    dataset[\"attention_mask\"][0]\n",
    ")\n",
    "\n",
    "# get the data collator\n",
    "data_collator = ReformerCollator(non_padded_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxDAHGu7rQ-E"
   },
   "source": [
    "Next, we will define our reformer model by defining the ReformerConfig. As can be seen we alternate between local attention layers and lsh attention layers to have a total of 6 layers. Also note that we factorize the num_buckets and use Axial Position Embeddings. For more insight on how the bucketing and Axial Position Embeddings work please refer to the Reformer docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"attention_head_size\": 64,\n",
    "    \"attn_layers\": [\"local\", \"lsh\", \"local\", \"lsh\", \"local\", \"lsh\"],\n",
    "    \"axial_pos_embds\": True,\n",
    "    \"sinusoidal_pos_embds\": False,\n",
    "    \"axial_pos_embds_dim\": [64, 192],\n",
    "    \"axial_pos_shape\": [512, 1024],\n",
    "    \"lsh_attn_chunk_length\": 64,\n",
    "    \"local_attn_chunk_length\": 64,\n",
    "    \"feed_forward_size\": 512,\n",
    "    \"hidden_act\": \"relu\",\n",
    "    \"hidden_size\": 256,\n",
    "    \"is_decoder\": True,\n",
    "    \"max_position_embeddings\": 524288,\n",
    "    \"num_attention_heads\": 2,\n",
    "    \"num_buckets\": [64, 128],\n",
    "    \"num_hashes\": 1,\n",
    "    \"vocab_size\": 320,\n",
    "    \"lsh_attention_probs_dropout_prob\": 0.0,\n",
    "    \"lsh_num_chunks_before\": 1,\n",
    "    \"lsh_num_chunks_after\": 0,\n",
    "    \"local_num_chunks_before\": 1,\n",
    "    \"local_num_chunks_after\": 0,\n",
    "    \"local_attention_probs_dropout_prob\": 0.025,\n",
    "    \"hidden_dropout_prob\": 0.025,\n",
    "}\n",
    "\n",
    "config = ReformerConfig(**config)\n",
    "model = ReformerModelWithLMHead(config)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vQcaAbwtQzD"
   },
   "source": [
    "Lastly, let's set up the training args. Note: these training settings have not throughly been tested and might be tuned for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj_bU848t6DS"
   },
   "source": [
    "# Change\n",
    "Transformers 4.x the TrainingArguments signature has changed:\\\n",
    "\t•\tevaluate_during_training → replaced by evaluation_strategy\\\n",
    "  \t•\tper_gpu_* → renamed to per_device_train_batch_size, per_device_eval_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-2db7d406bff3>:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\",                   # where to save checkpoints & logs\n",
    "    learning_rate=1e-3,\n",
    "    max_steps=2000,\n",
    "\n",
    "    per_device_train_batch_size=1,     # was per_gpu_train_batch_size\n",
    "    per_device_eval_batch_size=1,      # was per_gpu_eval_batch_size\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    # legacy eval flags:\n",
    "    do_eval=True,                      # run evaluation\n",
    "    eval_steps=50,                     # how often to evaluate\n",
    "    logging_steps=50,                  # how often to log\n",
    "\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,           # your tokenized+formatted Dataset\n",
    "    eval_dataset=dataset,            # optional, if you want eval\n",
    "    data_collator=data_collator,     # your ReformerCollator or HF collator\n",
    "    tokenizer=tokenizer,             # ensures `.save_model()` works properly\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNuuF6Nm0kSj"
   },
   "source": [
    "We define a simple \"accuracy\" metric to keep track of how many samples are correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    non_padded_indices = (pred.label_ids != -100)\n",
    "\n",
    "    # correctly shift labels and pred as it's done in forward()\n",
    "    labels = pred.label_ids[..., 1:][non_padded_indices[..., 1:]]\n",
    "    pred = np.argmax(pred.predictions[:, :-1], axis=-1)[non_padded_indices[..., :-1]]\n",
    "\n",
    "    acc = np.mean(np.asarray(pred == labels), dtype=np.float)\n",
    "    return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-83556ad4fd31>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manitejsri22\u001b[0m (\u001b[33manitejsri22-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250513_024413-pgnpfvbj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anitejsri22-student/huggingface/runs/pgnpfvbj' target=\"_blank\">./</a></strong> to <a href='https://wandb.ai/anitejsri22-student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anitejsri22-student/huggingface' target=\"_blank\">https://wandb.ai/anitejsri22-student/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anitejsri22-student/huggingface/runs/pgnpfvbj' target=\"_blank\">https://wandb.ai/anitejsri22-student/huggingface/runs/pgnpfvbj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,           # your tokenized+formatted Dataset\n",
    "    eval_dataset=dataset,            # optional, if you want eval\n",
    "    data_collator=data_collator,     # your ReformerCollator or HF collator\n",
    "    tokenizer=tokenizer,             # ensures `.save_model()` works properly\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"my-reformer-checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGvyapRNZVqz"
   },
   "source": [
    "The following code ensures that padding is handled properly during generation by giving the tokenizer a distinct PAD token (so padding isn’t treated as end-of-sequence), switching the model into evaluation mode, and producing an attention mask that distinguishes real tokens from padding. We then call model.generate(...) with that mask and appropriate sampling parameters to produce a coherent continuation of our prompt, and finally decode the output back into plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Later that day, he noadedroï as agight by sYou, manvenestay rez D wasI h man unar?” thatul, hadnhe“ing I butit?” d m cilL g arevaskoles will, And gnoiveherL isikov knowivevery knowlf B Ten areu in ButiveeuY And,im<s> sh ab abouticad neau’A B\n"
     ]
    }
   ],
   "source": [
    "# 1) Give the tokenizer a real PAD token (distinct from EOS)\n",
    "if tokenizer.pad_token is None or tokenizer.pad_token == tokenizer.eos_token:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    # expand the model embeddings to include the new PAD token\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 2) Switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# 3) Tokenize with padding so we get an attention_mask\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(\n",
    "        \"Later that day, he\",\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,            # pad to longest in batch (here just the prompt)\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 4) Generate, explicitly passing the attention_mask\n",
    "    generated = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],  # ensures the model knows what’s real vs pad\n",
    "        max_length=inputs[\"input_ids\"].shape[-1] + 100,\n",
    "        pad_token_id=tokenizer.pad_token_id,      # now a distinct PAD ID\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "# 5) Decode\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcXoLv0bgemi"
   },
   "source": [
    "Factual Question-Answering\n",
    "\n",
    "Why? Checks whether the model has learned and can recall facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a543daf8027>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     out = model.generate(\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Who wrote ‘Crime and Punishment’?\",\n",
    "    \"In what year was the Declaration of Independence signed?\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "for p in prompts:\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(\n",
    "        **inputs, max_length=inputs[\"input_ids\"].shape[-1]+20,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    answer = tokenizer.decode(out[0], skip_special_tokens=True)[len(p):].strip()\n",
    "    print(f\"> {p}\\n→ {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52Z2Ys2IgsTy"
   },
   "source": [
    "Cloze / Fill-in-the-Blank\n",
    "\n",
    "Why? Tests local consistency and the model’s ability to predict missing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Dostoyevsky’s most famous novel is Crime and ____.\n",
      "→ Dostoyevsky’s most famous novel is Crime and ____.uomomeul about\n",
      "\n",
      "> The mitochondrion is the powerhouse of the ____.\n",
      "→ The mitochondrion is the powerhouse of the ____.DLnight g\n",
      "\n",
      "> To be, or not to be, that is the ____.\n",
      "→ To be, or not to be, that is the ____. gzikeerhin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "templates = [\n",
    "    \"Dostoyevsky’s most famous novel is Crime and ____.\",\n",
    "    \"The mitochondrion is the powerhouse of the ____.\",\n",
    "    \"To be, or not to be, that is the ____.\"\n",
    "]\n",
    "for t in templates:\n",
    "    inputs = tokenizer(t, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_length=inputs[\"input_ids\"].shape[-1]+5)\n",
    "    fill = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    print(f\"> {t}\\n→ {fill}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvaCACf0h_W2"
   },
   "source": [
    "Long-Context Understanding\n",
    "\n",
    "Why? Verifies the Reformer really uses long context rather than only the last 512 tokens.\n",
    "Simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 1415 to 1472 to be a multiple of `config.chunk_length`: 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was tired?ilalodast fromlfastppingl of( baskoling\n"
     ]
    }
   ],
   "source": [
    "# 1) build a long context\n",
    "seed_sent = \"Alice was beginning to get very tired of sitting by her sister on the bank.\"\n",
    "context = \" \".join([seed_sent for _ in range(40)])  # ~2000 tokens\n",
    "question = \"Who was tired?\"\n",
    "prompt = context + \"\\n\\nQuestion: \" + question\n",
    "# 2) generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=False).to(model.device)\n",
    "out = model.generate(\n",
    "    **inputs, max_length=inputs[\"input_ids\"].shape[-1]+20,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True)[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvUbqzl6iLzN"
   },
   "source": [
    "Summarization of a Paragraph\n",
    "\n",
    "Why? Even without fine-tuning, we can see if LM can compress information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = (\n",
    "    \"In Russian literature, Fyodor Mikhailovich Dostoyevsky is famed for delving into human psychology. \"\n",
    "    \"Crime and Punishment explores guilt, redemption, and the moral dilemmas of its protagonist, Raskolnikov.\"\n",
    ")\n",
    "prompt = \"Summarize the following in one sentence:\\n\\n\" + paragraph\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "summary = model.generate(\n",
    "    **inputs, max_length=inputs[\"input_ids\"].shape[-1]+30,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(summary[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOdNPYRdiUGk"
   },
   "source": [
    "Adversarial / Nonsense Prompts\n",
    "\n",
    "Why? Gauges robustness and whether the model hallucinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Colorless green ideas sleep soundly.\n",
      "→ Colorless green ideas sleep soundly.hinia”DLnight g<s> gim guomomeul about“ had know\n",
      "\n",
      "> asdf qwer zxcv why do letters swim?\n",
      "→ asdf qwer zxcv why do letters swim?ia”DLnight g<s> gim guomomeul about“ had knowy\n",
      "\n",
      "> The square root of avocado is banana because ___.\n",
      "→ The square root of avocado is banana because ___.ight g<s> gim guomomeul about“ had knowyYou thatest butu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adversarials = [\n",
    "    \"Colorless green ideas sleep soundly.\",\n",
    "    \"asdf qwer zxcv why do letters swim?\",\n",
    "    \"The square root of avocado is banana because ___.\"\n",
    "]\n",
    "for p in adversarials:\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_length=inputs[\"input_ids\"].shape[-1]+20)\n",
    "    print(f\"> {p}\\n→ {tokenizer.decode(out[0], skip_special_tokens=True)}\\n\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
